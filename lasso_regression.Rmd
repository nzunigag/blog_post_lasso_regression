---
title: "R Notebook"
output: html_notebook
---

# Lasso Regression with R  

This markdown contained the example used in the blog post [Lasso Regression with R](https://nzunigag.github.io/Lasso/).

```{r}
library(glmnet)
set.seed(1234)

# Load Diabetes Data
X = read.csv('diabetesX.csv')
X = as.matrix(X) 
Y = read.csv('diabetesY.csv',header=F)
Y = Y[,1]

N <- 50 # Number of lambdas
fit <- glmnet(X, Y, alpha = 1, nlambda = N)
plot(fit, xvar = 'lambda', xlab = 'log(lambda)')
```

```{r}
# In-sample mean square prediction error (MSE)
Y.prediction <- predict.glmnet(fit, newx = X)
MSE <- array(NA, dim = N)
for (i in 1:N){
  MSE[i] <- mean((Y.prediction[,i] - Y)^2)
}

plot(log(fit$lambda), MSE, type = "l", xlab = 'log(lambda)')
```


```{r}
K_fold <- function(X, Y, K, lambda){
  # Compute the K-fold Cross-Validation
  # Input:
  # X = features (matrix M x P)
  # y = vector of observations (vector M)
  # K = number of folds (constant, typically 5 or 10)
  # lambda = set of complexity parameters (vector N)
  # Output:
  # MSE = Mean Square Error (vector N)
  
  # We split the data into K roughly equal-sized parts 
  # using an indexing function
  partition <- cut(1:dim(X)[1], breaks = K, labels=FALSE)
  # Randomization
  partition <- sample(partition)
  
  N <- length(lambda)
  MSE <- array(NA, dim = c(K, N))
  for (i in 1:K){
    validation <- partition == i  # Validation Kth part
    X.validation <- X[validation,]
    Y.validation <- Y[validation]
    train <- !validation  # Train part
    X.train <- X[train,]
    Y.train <- Y[train]
    fit <- glmnet(X.train, Y.train, alpha = 1, lambda = lambda)
    Y.prediction <- predict.glmnet(fit, newx = X.validation)
    for (j in 1:N){
      MSE[i, j] <- mean(((Y.prediction[,j] - Y.validation)^2))
    }
  }
  colMeans(MSE)
}
```



```{r}
# Applying the cross validation using K-fold function
K <- 5 
MSE_Kfold <- K_fold(X, Y, K, fit$lambda)

plot(log(fit$lambda), MSE, type = "l", xlab = 'log(lambda)')
points(log(fit$lambda), MSE_Kfold, type="l", col="blue",lty=2)
legend('topleft',legend = c('In-sample MSE','K-Fold CV'), 
       col=c("black", "blue"), lty=1:2, cex=0.8)
```

```{r}
# Sanity Check for K-fold CV
MSE_check <- cv.glmnet(X, Y, lambda = fit$lambda, nfolds = K)
plot(MSE_check)

```


```{r}
# Collin Mallows's Cp Statistic
Cp_Mallows <- function(X, Y, lambda){
  # Compute the Collin Mallows's Cp Statistic
  # Input:
  # X = features (matrix M x P)
  # y = vector of observations (vector M)
  # lambda = set of complexity parameters (vector N)
  # Output:
  # Cp = Collin Mallows's Cp Statistic (vector N)
  
  N <- length(lambda)
  M <- dim(X)[1]
  P <- dim(X)[2]
  # Obtain the unbiased sigma-square from the OLS
  fit <- glmnet(X, Y, alpha = 1, lambda = 0)
  Y.OLS <- predict.glmnet(fit, newx = X)
  sigma.square <- sum((Y - Y.OLS) ^ 2) / (M - P)
  # In-sample mean square prediction error
  fit <- glmnet(X, Y, alpha = 1, lambda = lambda)
  Y.prediction <- predict.glmnet(fit, newx = X)
  MSE <- array(NA, dim = N)
  for (i in 1:N){
    MSE[i] <- mean((Y.prediction[,i] - Y)^2)
  }
  # Estimate the Cp Statistics
  MSE + 2 * (fit$df / M) * sigma.square
}
```

```{r}
# Applying the Collin Mallows's Cp Statistic function
MSE_Cp <- Cp_Mallows(X, Y, fit$lambda)

plot(log(fit$lambda), MSE, type = "l", xlab = 'log(lambda)')
points(log(fit$lambda), MSE_Kfold, type="l", col="blue",lty=2)
points(log(fit$lambda), MSE_Cp, type="l", col="red")
legend('topleft',legend = c('In-sample MSE','K-Fold CV','Mallow Cp'), 
       col=c("black", "blue","red"), lty=1:2, cex=0.8)
```


